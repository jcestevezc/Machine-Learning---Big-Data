{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.master('local[*]').appName('textMining').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Is not the best choose for large data sets\n",
    "sms = spark.read.csv('sms.csv', sep=';', inferSchema=True, nullValue='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = sms.withColumnRenamed('_c0','id')\n",
    "sms = sms.withColumnRenamed('_c1','text')\n",
    "sms = sms.withColumnRenamed('_c2','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  1|Sorry, I'll call ...|    0|\n",
      "|  2|Dont worry. I gue...|    0|\n",
      "|  3|Call FREEPHONE 08...|    1|\n",
      "|  4|Win a 1000 cash p...|    1|\n",
      "|  5|Go until jurong p...|    0|\n",
      "|  6|Ok lar... Joking ...|    0|\n",
      "|  7|Free entry in 2 a...|    1|\n",
      "|  8|U dun say so earl...|    0|\n",
      "|  9|Nah I don't think...|    0|\n",
      "| 10|FreeMsg Hey there...|    1|\n",
      "| 11|Even my brother i...|    0|\n",
      "| 12|As per your reque...|    0|\n",
      "| 13|WINNER!! As a val...|    1|\n",
      "| 14|Had your mobile 1...|    1|\n",
      "| 15|I'm gonna be home...|    0|\n",
      "| 16|SIX chances to wi...|    1|\n",
      "| 17|URGENT! You have ...|    1|\n",
      "| 18|I've been searchi...|    0|\n",
      "| 19|I HAVE A DATE ON ...|    0|\n",
      "| 20|XXXMobileMovieClu...|    1|\n",
      "+---+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "#wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = wrangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+\n",
      "| id|                text|label|               words|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|  1|Sorry I'll call l...|    0|[sorry, i'll, cal...|\n",
      "|  2|Dont worry I gues...|    0|[dont, worry, i, ...|\n",
      "|  3| Call FREEPHONE now |    1|[call, freephone,...|\n",
      "|  4|Win a cash prize ...|    1|[win, a, cash, pr...|\n",
      "|  5|Go until jurong p...|    0|[go, until, juron...|\n",
      "|  6|Ok lar Joking wif...|    0|[ok, lar, joking,...|\n",
      "|  7|Free entry in a w...|    1|[free, entry, in,...|\n",
      "|  8|U dun say so earl...|    0|[u, dun, say, so,...|\n",
      "|  9|Nah I don't think...|    0|[nah, i, don't, t...|\n",
      "| 10|FreeMsg Hey there...|    1|[freemsg, hey, th...|\n",
      "| 11|Even my brother i...|    0|[even, my, brothe...|\n",
      "| 12|As per your reque...|    0|[as, per, your, r...|\n",
      "| 13|WINNER As a value...|    1|[winner, as, a, v...|\n",
      "| 14|Had your mobile m...|    1|[had, your, mobil...|\n",
      "| 15|I'm gonna be home...|    0|[i'm, gonna, be, ...|\n",
      "| 16|SIX chances to wi...|    1|[six, chances, to...|\n",
      "| 17|URGENT You have w...|    1|[urgent, you, hav...|\n",
      "| 18|I've been searchi...|    0|[i've, been, sear...|\n",
      "| 19|I HAVE A DATE ON ...|    0|[i, have, a, date...|\n",
      "| 20|XXXMobileMovieClu...|    1|[xxxmobilemoviecl...|\n",
      "+---+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms').transform(sms)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024).transform(wrangled)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features').fit(wrangled).transform(wrangled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|                text|label|               words|               terms|                hash|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|Sorry I'll call l...|    0|[sorry, i'll, cal...|[sorry, call, lat...|(1024,[138,344,37...|(1024,[138,344,37...|\n",
      "|  2|Dont worry I gues...|    0|[dont, worry, i, ...|[dont, worry, gue...|(1024,[53,233,329...|(1024,[53,233,329...|\n",
      "|  3| Call FREEPHONE now |    1|[call, freephone,...|   [call, freephone]|(1024,[138,396],[...|(1024,[138,396],[...|\n",
      "|  4|Win a cash prize ...|    1|[win, a, cash, pr...|[win, cash, prize...|(1024,[31,69,387,...|(1024,[31,69,387,...|\n",
      "|  5|Go until jurong p...|    0|[go, until, juron...|[go, jurong, poin...|(1024,[116,262,33...|(1024,[116,262,33...|\n",
      "|  6|Ok lar Joking wif...|    0|[ok, lar, joking,...|[ok, lar, joking,...|(1024,[449,572,66...|(1024,[449,572,66...|\n",
      "|  7|Free entry in a w...|    1|[free, entry, in,...|[free, entry, wkl...|(1024,[16,24,77,1...|(1024,[16,24,77,1...|\n",
      "|  8|U dun say so earl...|    0|[u, dun, say, so,...|[u, dun, say, ear...|(1024,[26,212,249...|(1024,[26,212,249...|\n",
      "|  9|Nah I don't think...|    0|[nah, i, don't, t...|[nah, think, goes...|(1024,[364,396,50...|(1024,[364,396,50...|\n",
      "| 10|FreeMsg Hey there...|    1|[freemsg, hey, th...|[freemsg, hey, da...|(1024,[112,163,17...|(1024,[112,163,17...|\n",
      "| 11|Even my brother i...|    0|[even, my, brothe...|[even, brother, l...|(1024,[41,319,367...|(1024,[41,319,367...|\n",
      "| 12|As per your reque...|    0|[as, per, your, r...|[per, request, me...|(1024,[8,16,60,18...|(1024,[8,16,60,18...|\n",
      "| 13|WINNER As a value...|    1|[winner, as, a, v...|[winner, valued, ...|(1024,[37,69,88,1...|(1024,[37,69,88,1...|\n",
      "| 14|Had your mobile m...|    1|[had, your, mobil...|[mobile, months, ...|(1024,[119,138,15...|(1024,[119,138,15...|\n",
      "| 15|I'm gonna be home...|    0|[i'm, gonna, be, ...|[gonna, home, soo...|(1024,[114,192,44...|(1024,[114,192,44...|\n",
      "| 16|SIX chances to wi...|    1|[six, chances, to...|[six, chances, wi...|(1024,[31,122,163...|(1024,[31,122,163...|\n",
      "| 17|URGENT You have w...|    1|[urgent, you, hav...|[urgent, won, wee...|(1024,[69,193,197...|(1024,[69,193,197...|\n",
      "| 18|I've been searchi...|    0|[i've, been, sear...|[searching, right...|(1024,[181,210,22...|(1024,[181,210,22...|\n",
      "| 19|I HAVE A DATE ON ...|    0|[i, have, a, date...|      [date, sunday]|(1024,[28,360],[1...|(1024,[28,360],[5...|\n",
      "| 20|XXXMobileMovieClu...|    1|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(1024,[9,45,87,13...|(1024,[9,45,87,13...|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms = tf_idf\n",
    "tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   47|\n",
      "|    0|       0.0|  987|\n",
      "|    1|       1.0|  124|\n",
      "|    0|       1.0|    3|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline \n",
    "\n",
    "# Break text into tokens at non-word characters\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\n",
    "\n",
    "# Apply the hashing trick and transform to TF-IDF\n",
    "hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol='hash')\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Create a logistic regression object and add everything to a pipeline\n",
    "logistic = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
