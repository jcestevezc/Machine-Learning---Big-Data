{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "1. Introduction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcestevezc/Machine-Learning-Big-Data/blob/master/Machine%20Learning%20on%20Spark/Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Xa80VU_pdF",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning on Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA1qCzzZ4A_U",
        "colab_type": "text"
      },
      "source": [
        "## 0. Methodology CRISP-DM and ASUM-DM\n",
        "\n",
        "![CRISP](https://github.com/jcestevezc/Machine-Learning-Big-Data/blob/master/Machine%20Learning%20on%20Spark/CRISP.png?raw=true)\n",
        "\n",
        "\n",
        "![ASUM](https://github.com/jcestevezc/Machine-Learning-Big-Data/blob/master/Machine%20Learning%20on%20Spark/ASUM.jpg?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNRXaXt-_pdH",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Creating the spark session and context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4pRDKB4_55T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6a138bd8-caed-46c1-e873-3cfc37448cf0"
      },
      "source": [
        "# ! pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxTM8dXu_pdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the PySpark module\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIJ3VksY_pdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Create SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Create SparkSession object\n",
        "spark = SparkSession.builder \\\n",
        "                    .master('local[*]') \\\n",
        "                    .appName('test') \\\n",
        "                    .getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YABscH0f_pdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "08054bc1-9e33-4379-835d-820f49e7db23"
      },
      "source": [
        "# What version of Spark?\n",
        "print(spark.version)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP6AcyPb_pdW",
        "colab_type": "text"
      },
      "source": [
        "### Read data from CSV file (infering schema) [Indice](#Indice)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20tba71z_pdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Is not the best choose for large data sets\n",
        "flights = spark.read.csv('/content/sample_data/flights_small.csv',\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA'\n",
        "                        )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBd9JQr__pdd",
        "colab_type": "text"
      },
      "source": [
        "### Read data from CSV file (defining schema)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z74LPeP_pde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Specify column names and types\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"text\", StringType()),\n",
        "    StructField(\"label\", IntegerType())\n",
        "])\n",
        "\n",
        "# Load data from a delimited file\n",
        "sms = spark.read.csv('https://raw.githubusercontent.com/jcestevezc/Machine-Learning-Big-Data/blob/master/Machine%20Learning%20with%20PySpark/sms.csv', sep=';', header=False, schema=schema)\n",
        "\n",
        "# Print schema of DataFrame\n",
        "sms.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5XzqLN2_pdi",
        "colab_type": "text"
      },
      "source": [
        "### Exploring data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwX6qhu-_pdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get number of records\n",
        "print(\"The data contain %d records.\" % flights.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufw8mUDL_pdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View the first five records\n",
        "flights.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDHbpOXL_pdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flights.describe().toPandas().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av3g2qhJ_pdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check column data types\n",
        "flights.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx-_T-h0_pdw",
        "colab_type": "text"
      },
      "source": [
        "### Selecting columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnNuhujJ_pdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flights_drop = flights.drop('distance','hour')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2stqJnue_pdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flights_select = flights.select('year','month')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Eg5KUi_pd2",
        "colab_type": "text"
      },
      "source": [
        "### Filtering out missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCOSAT-z_pd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flights.filter('arr_delay is null').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKs0AtuR_pd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flights_no_miss = flights.filter('arr_delay is null')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FazDDK1R_pd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flights_no_miss_2 = flights.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyXJ_4Ty_peB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Terminate the cluster\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6dFC7QK_peE",
        "colab_type": "text"
      },
      "source": [
        "### Column manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gbj9DgZ_peF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required function\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
        "                    .drop('mile')\n",
        "flights_km.head()\n",
        "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
        "flights_km = flights_km.withColumn('label', (flights_km.delay >=15).cast('integer'))\n",
        "\n",
        "# Check first five records\n",
        "flights_km.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am5DYZFO_peI",
        "colab_type": "text"
      },
      "source": [
        "### Indexing categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD_JhAAp_peI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create an indexer\n",
        "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
        "\n",
        "# Indexer identifies categories in the data\n",
        "indexer_model = indexer.fit(flights)\n",
        "\n",
        "# Indexer creates a new column with numeric index values\n",
        "flights_indexed = indexer_model.transform(flights)\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWH63ZT1_peL",
        "colab_type": "text"
      },
      "source": [
        "### Assembling columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVvtJ2ox_peM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an assembler object\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=['mon', \n",
        "               'dom' , \n",
        "               'dow',\n",
        "               'carrier_idx',\n",
        "               'org_idx',\n",
        "               'km',\n",
        "               'depart',\n",
        "               'duration'\n",
        "              ], outputCol='features')\n",
        "\n",
        "# Consolidate predictor columns\n",
        "flights_assembled = assembler.transform(flights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKPcEagL_peP",
        "colab_type": "text"
      },
      "source": [
        "### Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFT0N8aS_peP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=17)\n",
        "\n",
        "# Check that training set has around 80% of records\n",
        "training_ratio = flights_train.count() / flights.count()\n",
        "print(training_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUrDwZAu_peS",
        "colab_type": "text"
      },
      "source": [
        "### Build a Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8oKP80p_peT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Decision Tree Classifier class\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Create a classifier object and fit to the training data\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_model = tree.fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and take a look at the predictions\n",
        "prediction = tree_model.transform(flights_test)\n",
        "prediction.select('label', 'prediction', 'probability').show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF-20Y1T_peV",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSsSRKFw_peV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a confusion matrix\n",
        "prediction.groupBy('label', 'prediction').count().show()\n",
        "\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP) \n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}